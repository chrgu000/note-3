递归神经网络
人类不会每秒从头开始思考。在阅读文章时，会根据对以前的理解了解每个词汇。并丢掉所有东西，然后再次从头开始思考。以前的东西依然存在。
传统的神经网络无法做到这一点。例如，假设想分类电影中每个点发生的事件类型。目前还不清楚传统神经网络如何利用其对电影中之前事件的推理来告知后来的事件。
递归神经网络解决了这个问题。网络中的循环，让信息持久地传递下去。
在上图中，神经网络A查看一些输入xt并输出一个值ht。循环允许信息从网络的一个步骤传递到下一个步骤。



输入门 it ：控制当前输入和前一步输出进入新的cell的信息量；
忘记门 ft ：决定哪些信息需要舍弃；
cell状态更新 ct ：计算下一个时间戳的状态使用经过们处理的前一状态和输入；
输出门 ot ：计算cell的输出；
最终LSTM的输出 yt ：使用一个对当前状态的softmax变换进行重变换。

其中 W 代表各个权重矩阵，如 Wix 是输入门到输出的权重矩阵， b 代表偏置向量，如 bi 是输入门的偏置向量， σ 是sigmoid函数，i,f,o,c分别代表输入门，忘记门，输出门以及cell状态更新向量，m是与i,f,o,c具有相同大小的输出向量，☉代表点乘，g和h分别为cell的输入输出激活函数，一般为tanh，Φ代表最终的LSTM输出激活函数，一般为softmax。

上图是一个已经在时间维度上展开(unroll)的Encoder-Decoder模型，其输入序列是”ABC”，输出序列是”WXYZ”，其中”<EOS>”是句子结束符。该模型由两个RNN组成：第1个RNN接受输入序列”ABC”并在读取到<EOS>时终止接受输入，并输出一个向量作为”ABC”这个输入项链的语义表示向量，此过程称为”Encoder”；第二个RNN接受第一个RNN产生的输入序列的语义向量，并且每个时刻t输出词的概率都与前t-1时刻的输出有关。

Encoder过程很简单，直接使用RNN（一般用LSTM）进行语义向量生成：
ht=f(xt,ht−1)

c=ϕ(h1,...,hT)
其中f是非线性激活函数， ht−1 是上一隐节点输出， xt 是当前时刻的输入。向量c通常为RNN中的最后一个隐节点(h, Hidden state)，或者是多个隐节点的加权和。
该模型的decoder过程是使用另一个RNN通过当前隐状态 ht 来预测当前的输出符号 yt ，这里的 ht 和 yt 都与其前一个隐状态和输出有关：
ht=f(ht−1,yt−1,c)

P(yt|yt−1,...,y1,c)=g(ht,yt−1,c)

Encoder-Decoder模型对于目标句子Y中每个单词的生成过程如下：
y1=f(c)

y2=f(c,y1)

y3=f(c,y1,y2)
其中f是decoder的非线性变换函数，由此可知，不论生成哪个单词，使用的语义向量都是c，而语义向量c是由句子X的每个单词经过Encoder编码而成的，也就意味着句子X中的单词对生成任意目标单词的影响力是相同的。Attention Model会对输入序列X的不同单词分配不同的概率，
此时目标单词生成过程如下：
y1=f(c1)

y2=f(c2,y1)

y3=f(c3,y1,y2)
其中c(i)对应输入序列X不同单词的概率分布，其计算公式为：
ci=∑ni=1αijhj

其中n为输入序列的长度， hj 是第j时刻的隐状态，而权重 αij 用如下公式计算：
αij=exp(eij)∑nk=1exp(eik)