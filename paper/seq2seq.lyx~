#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language chinese-simplified
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
神经网络序列到序列的学习
\end_layout

\begin_layout Abstract
深度神经网络（DNNs）是强大的模型，在困难的学习任务上取得了出色的表现。尽管DNNs在很大的标签训练集上工作的很好，但是它们不能用于序列到序列的映射。在本文中
，我们提出了一般端到端 的方法，对序列结构做出最小假设。我们的方法使用一个多层长短期记忆网络（LSTM）来把输入序列映 射到一个固定维度的向量，然后另一个深度L
STM网络从这个向量中解码出目标序列。我们的主要结果是基于WMT-14数据集做的英语到法语的翻译，这个由LSTM产生的结果在整个测试数据集上达到了34.8的BLE
U得分，BLEU得分是超出词汇单词的惩罚（错误率）。另外，LSTM处理长句子并不困难。作为比较，一个基于短语的SMT系统在相同的数据集上获得了33.3的BLEU得
分。当我们使用LSTM来重跑由之 前SMT系统产生的1000个假设，BLEU得分增加到了36.5，很接近先前的最新状态。LSTM也能学习合理 的短语和句子表述，对
词序很敏感，对主动和被动语态相对不变（不太敏感）。我们发现把源句子（不是目标句子）的序列翻转后（比如a,b,c,d -> d,c,b,a）输入，LSTM的表现明
显提高了，因 为这样做引入了源和目标语句之间的许多短期依赖关系，从而使优化问题更加容易
\end_layout

\begin_layout Section
介绍
\end_layout

\begin_layout Standard
深度神经网络（DNNs）是非常强大的机器学习模型，注意在诸如语音识别[13,7]和视觉对象识别 [19,6,21,20]等难题上实现卓越的性能。Dnns功能强大
，因为对于少量步骤，它们可以执行任意并行计算。Dnns功能的一个令人惊讶的例子是它们仅使用2个二次大小的隐藏层对n位数进行排序的能力[27]。所以，尽管神经网络
与传统的统计模型相关，但它们学习了复杂的计算。此外，只要标记的训练集具有足 够的信息来指定网络的参数，就可以用监督反向传播来训练。因此，如果存在获得良好结果的大
数据集的参数设置（例如，因为人可以非常迅速地解决任务），则监督反向传播将找到这些参数病解决问题。
\end_layout

\begin_layout Standard
尽管它们具有灵活性和强大的功能，但DNNs只能用于输入和目标可以用维数固定的矢量进行合理编码的问题。这是一个重要的限制，因为许多重要问题最好用序列长度未知的序列
来表示。例如，语音识别 和机器翻译是序列问题。同样，问题回答也可以被看作是将表示问题的单词序列映射到表示答案的单词序列。因此清楚的是，学习将序列映射到序列的领域
独立方法将是有用的。
\end_layout

\begin_layout Standard
序列对DNNs提出了挑战，因为它们要求输入和输出的维度是已知的并且是固定的。在本文中，我们 展示了一个简单长短期记忆（LSTM）体系结构[16]的直接应用可以解
决序列问题的一般序列问题。这个想法是使用一个LSTM来读取输入序列，一次一个时间步，以获得大的固定维向量表示，然后使用另一个从该向量提取输出序列（图1）。第二个
LSTM本质上是一个递归的神经网络语言模型[28,23,30]，除了它是以输入序列为条件的。由于输入和它们相应输出之间的相当大的时间依赖性（图1），LSTM成功
处理 长时间依赖关系数据的能力使其成为该应用的自然选择。
\end_layout

\begin_layout Standard
已经有许多相关的尝试来解决用神经网络对序列学习问题进行排序的一般序列。我们的方法与Kalchbrenner和Blunsom密切相关[18]，他们首先将整个输入句
子映射为向量，并且与Cho等人非常相似[5]。Graves[10]引入了一种新颖的可区分关注机制，它允许神经网络将注意力集中在输入的不同 部分，并且这一想法的一
个优雅变体被Bahdanau等人成功应用于机器翻译[2]。连接主义序列分类是将 序列映射到具有神经网络的序列的另一种流行技术，尽管它假设输入和输出之间是单调对齐
的[11]。 
\end_layout

\begin_layout Standard
这项工作的主要成果如下。在WMT'14英语到法语翻译任务中，我们使用简单的从左到右的波束搜索 解码器，通过从5个深层（全部380m参数）的集合直接提取翻译来获得
34.81的BLEU分数。这是迄今为止通过大型神经网络进行直接翻译所取得的最佳结果。相比之下，这个数据集上SMT基线的BLEU得分为 33.30[29]。这个34.8
1BLEU得分是由一个词汇为80k词的词条实现的，因此只要参考翻译包含一个未被这80k涵盖的词，分数就会受到惩罚。这个结果表明，相对未优化的神经网络结构具有很大
的改进空间，优于基于短语的SMT系统。
\end_layout

\begin_layout Standard
最后，我们使用LSTM在相同的任务中重新公开了SMT基线的1000个最佳列表[29]。通过这样做，我们获得了36.5的BLEU得分，其通过3.2个BLEU点提高了基
线并接近先前的最新状态（37.0[9]）。
\end_layout

\begin_layout Standard
令人惊讶的是，尽管最近有其他研究人员使用相关体系结构[26]，但lstm并没有遭受很长的句子。我们能够在长句中表现得很好，因为翻译源句子中的单词顺序，而不是训练
和测试集中的目标句子。通过这样做，我们引入了许多使得优化问题更简单的短期依赖性（参见第2节和第3.3节）。因此，sgd可以学习那些对长句没有困难的词典。翻译源句中
单词的简单诀窍是这项工作的关键技术贡献之一。
\end_layout

\begin_layout Standard
LSTM的一个有用属性是它学习将可变长度的输入句子映射为固定维向量表示。由于翻译往往是对源句的解释，因此翻译目标鼓励LSTM找到能够捕捉其含义的句子表示法，因为
具有相似含义的句子彼此接近，而不同的句子含义将会很远。定性评估支持这一说法，表明我们的模型意识到了词序，并且对于主动和被动语态都是不变的。
\end_layout

\begin_layout Section
模型
\end_layout

\begin_layout Standard
递归神经网络（rnn）[31,28]是前馈神经网络对序列的自然泛化。给定输入序列（x1，...，xt）， 标准rnn通过迭代以下等式来计算输出序列（y1，...，yt）：
\begin_inset Formula 
\[
h_{t}=sigm\left(W^{hx}x_{t}+W^{hh}h_{t-1}\right)
\]

\end_inset


\begin_inset Formula 
\[
y_{t}=W^{yh}h_{t}
\]

\end_inset


\end_layout

\begin_layout Standard
只要提前知道输出之间的对齐，rnn就可以轻松地将序列映射到序列。然而，对于复杂和非单调关系的 输入和输出序列长度不同的问题，还不清楚如何解决。
\end_layout

\begin_layout Standard
一个简单的通用序列学习策略是将输入序列映射到一个固定大小的载体上，然后用另一个rnn将载体映射到目标序列（Cho等人[5]也采用了这种方法）。尽管原则上它可以工
作，因为rnn提供了所有的相关信息，但由于长期的相关性[14，4]（图1）[16,15]，难以训练rnns。然而，长时间的短期记忆（lstm）[16]已知可以学
习长时间的时间依赖性问题，所以在这种情况下，lstm可能会成功。
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "lyxtutorial"

\end_inset

The lyx Tutorial,
\end_layout

\end_body
\end_document
