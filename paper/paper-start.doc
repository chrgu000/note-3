<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="http://www.nongnu.org/elyxer/">
<meta name="create-date" content="2018-03-13">
<link rel="stylesheet" href="http://elyxer.nongnu.org/lyx.css" type="text/css" media="all">
<title>基于seq2seq的聊天机器人算法研究</title>
</head>
<body>
<div id="globalWrapper">
<h1 class="title">
基于seq2seq的聊天机器人算法研究
</h1>
<h1 class="Section">
<a class="toc" name="toc-Section-1">1</a> 题目背景和意义
</h1>
<div class="Unindented">
本文论文题目：基于seq2seq的聊天机器人算法研究
</div>
<div class="Indented">
选题的背景：社近年来，聊天机器人受到了学术界和工业界的广泛关注。一方面，聊天机器人是图灵测试的一种实现方式，而图灵测试是人工智能领域王冠上的明珠；另一方面，微软推出了基于情感计算的聊天机器人小冰，百度推出了用于交互式搜索的聊天机器人小度，进而推动了聊天机器人产品化的发展。聊天机器人系统可以看作是机器人产业与“互联网+”的结合，符合国家的科研及产业化发展方向。
</div>
<div class="Indented">
选题的意义：聊天机器人是未来各种服务的入口，尤其是移动端APP应用及可穿戴设备场景下提供各种服务的服务入口，这类似于Web端搜索引擎的入口作用。将来很可能通过语音助手绕过目前的很多APP直接提供各种服务，比如查询天气、定航班、订餐、智能家居的设备控制、车载设备的语音控制等等，目前大多采用独立APP形式来提供服务，而将来很多以APP形式存在的应用很可能会消失不见，直接隐身到语音助手背后。
</div>
<div class="Indented">
题目理论研究价值或运用价值：
</div>
<div class="Indented">
聊天机器人作为未来各种应用服务的入口，其市场影响力毫无疑问是巨大的，随着物联网的发展，全球将会出现上百亿的设备，深度学习所赋予的智能将使得设备和应用更具竞争力和市场占有率。
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-2">2</a> 国内外研究现状
</h1>
<div class="Unindented">
基于深度学习技术的神经翻译技术（NMT）最大的优点在于：
</div>
<div class="Indented">
1、采用一种端到端（end-to-end）的结构，不在需要人为的去抽取特征；
</div>
<div class="Indented">
2、网络结构设计简单，不需要进行词语切分、词语对齐、句法树设计等复杂的设计工作。同时，这一方法的缺点也很明显：
</div>
<div class="Indented">
1、可解释性差，以seq2seq为例，很难解释和理解隐层输出，即encoder输出具体的物理意义；
</div>
<div class="Indented">
2、训练复杂度高，耗时耗力。深度学习训练样本量往往多大亿计，训练一个模型需要专门得GPU集群，花费几天甚至一周时间才能得出一个结果，模型的迭代更新非常慢。虽然NMT有它的不足，但总的来说还是利远远大于弊
</div>
<div class="Indented">
现在研究关于语言交互的bot有三个主要方面： 
</div>
<div class="Indented">
一方面人在主攻对话系统，微软的paper比较多，例如文献<span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1">1</a>]</span>
</div>
<div class="Indented">
另一拨人主要在做QA，或者QA相关的集成，这部分亚马逊相关的比较多，alexa prize相关的很多文章 都有这样的感觉，例如文献<span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2">2</a>]</span>
</div>
<div class="Indented">
还有一拨人主要在做chatbot之类的，例如文献<span class="bibcites">[<a class="bibliocite" name="cite-3" href="#biblio-3">3</a>]</span>
</div>
<div class="Indented">
以上三方面是最前沿的研究进展，而本课题的研究内容主要参考文献<span class="bibcites">[<a class="bibliocite" name="cite-4" href="#biblio-4">4</a>]</span>，基于sequence to sequence生成模型。
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-3">3</a> 主要内容
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.1">3.1</a> 主要内容：
</h2>
<div class="Unindented">
在文献<span class="bibcites">[<a class="bibliocite" name="cite-4" href="#biblio-4">4</a>]</span>中，主要任务是基于WMT-14数据集做的英语到法语的翻译，这为之后的基于深度学习的Google翻译提供了理论基础。
</div>
<div class="Indented">
LSTM模型：
</div>
<div class="Indented">
<img class="embedded" src="Selection_001.png" alt="figure Selection_001.png" style="max-width: 1320px; max-height: 442px;">

</div>
<div class="Indented">
LSTM的目标是估计条件概率p（y1，...，yt’| x1，...，xt），其中（x1，...，xt）是输入序列，并且y1, . . . , yT′是它的相应输出序列，其长度可能不同于T。LSTM通过首先获得由LSTM的最后一个隐藏状态给出的输入序列（x1，...，xt）的固定维数表示，然后计算y1, . . . , yT′的概率，来计算该条件概率。y1, . . . , yT′具有标准的LSTM-lm公式，其初始隐藏状态被设置为表示x1，...，xt<div class="formula">
<i>p</i><span class="symbol">(</span><i>y</i><sub>1</sub>, ..., <i>y</i><sub><i>T</i><sup>’</sup></sub>|<i>x</i><sub>1</sub>, ..., <i>x</i><sub><i>T</i></sub><span class="symbol">)</span> = <span class="limits"><span class="limit">∏</span></span><sub><i>t</i> = 1</sub><sup><i>T</i><sup>’</sup></sup><i>p</i><span class="symbol">(</span><i>y</i><sub><i>t</i></sub>|<i>v</i>, <i>y</i><sub>1</sub>, ..., <i>y</i><sub><i>t</i> − 1</sub><span class="symbol">)</span>
</div>

</div>
<div class="Indented">
在该等式中，词汇表中的每个词都用softmax表示每个p（yt | v，y1，...，yt-1）分布。 我们使用Graves [10]中的LSTM公式。请注意，我们需要每个句子以特殊的句尾符号“&lt;EOS&gt;”结尾，这使模型能够定义在所有可能长度的序列上的分布。总体方案如图1所示，其中所示的LSTM计算“a”，“b”，“c”，“&lt;EOS&gt;”的表示，然后用这个表示来计算“w”，“x” “y”，“z”，“&lt;EOS&gt;”。
</div>
<div class="Indented">
神经网络的端到端训练的进展已经在许多领域取得了显着的进步，如语音识别，计算机视觉和语言处理。最近的研究表明神经网络不仅可以做简单分类，还可以用来绘制复杂 结构到其他复杂结构。
</div>
<div class="Indented">
文献<span class="bibcites">[<a class="bibliocite" name="cite-5" href="#biblio-5">5</a>]</span>则是在文献<span class="bibcites">[<a class="bibliocite" name="cite-4" href="#biblio-4">4</a>]</span>的基础上做的会话模型。其优势在于它可以进行端对端培训，因此需要的手工规则少得多。模型可以从领域特定数据集中提取知识，也可以从电影字幕的大型，嘈杂和普通的主要数据集中提取知识。
</div>
<div class="Indented">
文献<span class="bibcites">[<a class="bibliocite" name="cite-5" href="#biblio-5">5</a>]</span>主要在2个数据集上进行了测试，分别是IT帮助台故障诊断数据集和开放域电影抄本数据集。
</div>
<div class="Indented">
实验结果如下：
</div>
<div class="Indented">
对话1：VPN问题 
</div>
<div class="Indented">
描述问题: 我访问vpn遇到问题
</div>
<div class="Indented">
<img class="embedded" src="Selection_003.png" alt="figure Selection_003.png" style="max-width: 765px; max-height: 1414px;">

</div>
<div class="Indented">
以上的结果显示机器人很好地解决了客户的问题。
</div>
<div class="Indented">
还有一个有意思的实验是基于常识的：
</div>
<div class="Indented">
<img class="embedded" src="Selection_011.png" alt="figure Selection_011.png" style="max-width: 692px; max-height: 1102px;">

</div>
<div class="Indented">
机器人对于哲学和道德问题也有一定的见解（数据集支持）
</div>
<div class="Indented">
<img class="embedded" src="Selection_014.png" alt="figure Selection_014.png" style="max-width: 686px; max-height: 651px;">

</div>
<div class="Indented">
<img class="embedded" src="Selection_015.png" alt="figure Selection_015.png" style="max-width: 832px; max-height: 1191px;">

</div>
<div class="Indented">
当然，有些问题也会出错：
</div>
<div class="Indented">
<img class="embedded" src="Selection_016.png" alt="figure Selection_016.png" style="max-width: 594px; max-height: 248px;">

</div>
<h1 class="Section">
<a class="toc" name="toc-Section-4">4</a> 设计方法
</h1>
<div class="Unindented">
以上的结果是令人鼓舞的，基于以上论文的学习，接下来的工作内容如下：
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.1">4.1</a> 准备数据
</h2>
<div class="Unindented">
中文电影对白语料或保险行业问答开放数据集
</div>
<div class="Indented">
第一个语料噪音比较大，许多对白问答关系没有对应好，第二个语料由现实世界的用户提出，高质量的答案由具有深度领域知识的专业人士提供。 问答语料是从原始英文数据翻译过来，未经其他处理的。
</div>
<div class="Indented">
时间充裕可以两个一起尝试。
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.2">4.2</a> 建立模型
</h2>
<div class="Unindented">
参考github开源模型，语言选择tensorflow 或 keras。其中重点理解LSTM和seq2seq模型。
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.3">4.3</a> 训练
</h2>
<div class="Unindented">
使用准备好的模型训练数据集。
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.4">4.4</a> 预测
</h2>
<div class="Unindented">
使用训练完成的模型生成答案并输出。时间充裕考虑把结果做出api形式供web或app调用。
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-5">5</a> 进度计划
</h1>
<div class="Unindented">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
设计（论文）各阶段名称
</td>
<td align="center" valign="top">
起 止 日 期
</td>

</tr>
<tr>
<td align="center" valign="top">
1
</td>
<td align="center" valign="top">
确定选题，阅读相关文献撰写文献综述，并翻译英文论文
</td>
<td align="center" valign="top">
2017-1-1—2017-2-24
</td>

</tr>
<tr>
<td align="center" valign="top">
2
</td>
<td align="center" valign="top">
选择合适模型，选择数据集，测试数据，完成开题
</td>
<td align="center" valign="top">
2017-2-25—2017-3-16
</td>

</tr>
<tr>
<td align="center" valign="top">
3
</td>
<td align="center" valign="top">
研究改进算法，测试代码，提升交互性，改进算法
</td>
<td align="center" valign="top">
2017-3-16—2017-4-12
</td>

</tr>
<tr>
<td align="center" valign="top">
4
</td>
<td align="center" valign="top">
算法优化、系统测试
</td>
<td align="center" valign="top">
2017-4-13—2017-5-20
</td>

</tr>
<tr>
<td align="center" valign="top">
5
</td>
<td align="center" valign="top">
整理资料、撰写论文、准备答辩
</td>
<td align="center" valign="top">
2017-5-21—2017-6-7
</td>

</tr>

</table>

</div>
<a class="toc" name="References"></a><h1 class="biblio">
References
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-1">1</a>] </span>[1]Xiujun Li, End-to-End Task-Completion Neural Dialogue Systems, 2017
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-2">2</a>] </span>[2]Huiting Liu, RubyStar: A Non-Task-Oriented Mixture Model Dialog System, 2017
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-3">3</a>] </span>[3]Jiwei Li, Adversarial Learning for Neural Dialogue Generation, 2017
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-4">4</a>] </span>[4]I Sutskever, O Vinyals, QV Le. Sequence to Sequence Learning with Neural Networks - Advances in neural information Processing Systems 27(NLPS 2014), 2014
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-5">5</a>] </span>O Vinyals, Q Le - A Neural Conversational Model, - arXiv preprint arXiv:1506.05869, 2015
</p>

</div>
</body>
</html>
